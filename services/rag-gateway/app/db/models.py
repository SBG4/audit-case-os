"""
SQLAlchemy models for RAG Gateway database.

Following PROJECT_SPEC.xml schema definitions for:
- documents: Document metadata from IRIS cases
- chunks: Text chunks with vector embeddings
- sync_jobs: Sync operation tracking
"""

from datetime import datetime
from typing import Optional
from uuid import uuid4

from sqlalchemy import (
    Column,
    Integer,
    String,
    BigInteger,
    Text,
    Boolean,
    TIMESTAMP,
    ForeignKey,
    Index,
)
from sqlalchemy.dialects.postgresql import JSONB, UUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector

Base = declarative_base()


class Document(Base):
    """
    Metadata for ingested documents from IRIS cases.

    Stores file information and references to source case.
    Actual file content stored in MinIO.
    """
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True, autoincrement=True)
    case_id = Column(Integer, nullable=False, index=True)
    document_name = Column(String(512), nullable=False)
    document_type = Column(String(50))  # MIME type
    file_size = Column(BigInteger)
    file_hash = Column(String(64), index=True)  # SHA-256 hash for deduplication
    storage_path = Column(Text)  # MinIO object path
    uploaded_at = Column(TIMESTAMP, server_default=func.now())
    doc_metadata = Column(JSONB)  # IRIS evidence ID, tags, custom fields

    # Relationships
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")

    def __repr__(self) -> str:
        return f"<Document(id={self.id}, case_id={self.case_id}, name='{self.document_name}')>"


class Chunk(Base):
    """
    Text chunks with embeddings for semantic search.

    Documents are split into fixed-size chunks (512 tokens) with overlap (128 tokens).
    Each chunk has a 384-dimensional embedding generated by Sentence Transformers.
    """
    __tablename__ = "chunks"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid4)
    document_id = Column(Integer, ForeignKey("documents.id", ondelete="CASCADE"), nullable=False, index=True)
    case_id = Column(Integer, nullable=False, index=True)  # Denormalized for fast filtering
    chunk_index = Column(Integer, nullable=False)  # Sequential position in document
    content = Column(Text, nullable=False)  # Raw text content
    embedding = Column(Vector(384))  # Sentence Transformer embedding
    token_count = Column(Integer)  # Number of tokens in chunk
    chunk_metadata = Column(JSONB)  # Page number, section title, etc.
    created_at = Column(TIMESTAMP, server_default=func.now())

    # Relationships
    document = relationship("Document", back_populates="chunks")

    def __repr__(self) -> str:
        return f"<Chunk(id={self.id}, document_id={self.document_id}, chunk_index={self.chunk_index})>"


# Create vector index for similarity search
# Note: This is created in migration SQL, not via SQLAlchemy
# CREATE INDEX idx_chunks_embedding_ivfflat ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);


class SyncJob(Base):
    """
    Track sync operations from IRIS to RAG database.

    Records the status and progress of background sync jobs.
    Used for monitoring, debugging, and providing status to users.
    """
    __tablename__ = "sync_jobs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    case_id = Column(Integer, nullable=False, index=True)
    status = Column(String(20), nullable=False, index=True)  # pending, running, completed, failed
    started_at = Column(TIMESTAMP)
    completed_at = Column(TIMESTAMP)
    documents_synced = Column(Integer, default=0)
    chunks_created = Column(Integer, default=0)
    error_message = Column(Text)
    job_metadata = Column(JSONB)  # Additional context (force_reindex, user, etc.)

    def __repr__(self) -> str:
        return f"<SyncJob(id={self.id}, case_id={self.case_id}, status='{self.status}')>"


class SearchHistory(Base):
    """
    Audit log of search queries for analytics.

    Optional table for tracking search patterns and performance metrics.
    Can be used for improving search quality and user experience.
    """
    __tablename__ = "search_history"

    id = Column(Integer, primary_key=True, autoincrement=True)
    query_text = Column(Text, nullable=False)
    case_id = Column(Integer)  # Optional filter
    results_count = Column(Integer)
    executed_at = Column(TIMESTAMP, server_default=func.now(), index=True)
    user_id = Column(String(255))  # From IRIS API key
    response_time_ms = Column(Integer)

    def __repr__(self) -> str:
        return f"<SearchHistory(id={self.id}, query='{self.query_text[:50]}...', results={self.results_count})>"
